{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CzlhCISmU8yZ"
   },
   "source": [
    "# Setting up notebook & downloading data and libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "executionInfo": {
     "elapsed": 1200,
     "status": "ok",
     "timestamp": 1601459559120,
     "user": {
      "displayName": "Арсен Ринатович Кужамуратов",
      "photoUrl": "",
      "userId": "11945829493768460540"
     },
     "user_tz": -180
    },
    "id": "YukFNJdnJ-OK",
    "outputId": "e4e9a15e-936b-4e59-9024-980da8e683b4"
   },
   "outputs": [],
   "source": [
    "#  Change directory to folder with 'cc.ru.300.vec' pretrained embeddings for rus_corpora\n",
    "#  fastText(ru) \n",
    "#  embeddings was taken from \n",
    "#  https://github.com/yandexdataschool/nlp_course/blob/2020/week01_embeddings/homework.ipynb\n",
    "#  download at https://yadi.sk/d/3yG0-M4M8fypeQ\n",
    "#  train.csv from competition page\n",
    "#  test.csv from competition page"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "kJ3pxxGQGGpd"
   },
   "outputs": [],
   "source": [
    "# Downloading necessary libraries\n",
    "# Common\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import itertools\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "# NLP\n",
    "import gensim\n",
    "from gensim.models import KeyedVectors\n",
    "import nltk\n",
    "\n",
    "# ML\n",
    "from sklearn.model_selection import train_test_split\n",
    "import torch, torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from tqdm import tnrange\n",
    "from sklearn.metrics import f1_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 111
    },
    "executionInfo": {
     "elapsed": 1416,
     "status": "ok",
     "timestamp": 1601459562530,
     "user": {
      "displayName": "Арсен Ринатович Кужамуратов",
      "photoUrl": "",
      "userId": "11945829493768460540"
     },
     "user_tz": -180
    },
    "id": "223FYHrGGGph",
    "outputId": "3d959427-22b6-48fd-d7d0-e23c1e04ad58"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>class</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>В Синьцзян-Уйгурском автономном районе Китая С...</td>\n",
       "      <td>true</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Режиссер Дэвид Линч один из создателей телесер...</td>\n",
       "      <td>true</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text class\n",
       "0  В Синьцзян-Уйгурском автономном районе Китая С...  true\n",
       "1  Режиссер Дэвид Линч один из создателей телесер...  true"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = pd.read_csv('train.csv', sep='\\t')\n",
    "# data.drop('Unnamed: 0', axis='columns', inplace=True)\n",
    "data.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23883\n",
      "В Синьцзян-Уйгурском автономном районе Китая СУАР уйгурам будут давать по 10 тысяч юаней более 162 тысячи долларов за брак с представителями титульной нации  ханьцами. Местные власти сообщили об этом на своем официальном сайте передает South China Morning Post.Из-за процветающих в районе сепаратистских настроений Синьцзян давно является головной болью для Пекина. Снять напряженность и укрепить связи между местным населением и ханьцами власти Китая планируют с помощью программы поддержки межэтнических браков.Как отмечает издание для СУАР 10 тысяч юаней  это большие деньги учитывая что средний годовой доход на душу населения составляет всего 75 тысяч юаней более 12 тысячи долларов.Материалы по теме1117 13 июня 2014Великая антитеррористическая  стенаНа борьбу с внутренними врагами КНР тратит больше средств чем на оборону страныПомимо этого смешанные пары за каждый год прожитый в гармонии друг с другом будут получать по 50 тысяч юаней более 8 тысяч долларов. Дети рожденные в браке между уйгурами и ханьцами смогут бесплатно посещать детский сад и учиться в средней школе. При поступлении ребенка в институт правительство выделит семье грант в 5 тысяч юаней 814 долларов. Супруги также получат субсидии на здравоохранение.Однако несмотря на всевозможные льготы с момента появления закона в начале августа этого года ни один брак пока не был зарегистрирован. Всего по информации газеты на текущий момент заключено 54 брака между уйгурами и ханьцами. Однако эти пары никаких льгот не получают. К тому же браки между уйгурами и ханьцами как сообщает издание обычно заканчиваются разводом.Уйгуры составляют более 72 процентов населения СУАР китайцы-ханьцы  менее 26 процентов.\n"
     ]
    }
   ],
   "source": [
    "# Data stats\n",
    "print(len(data))\n",
    "i = 0\n",
    "print(data['text'][i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "text     23878\n",
       "class    23883\n",
       "dtype: int64"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Data stats\n",
    "data.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sEisbM1MRbef"
   },
   "source": [
    "# NLP part"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 71
    },
    "executionInfo": {
     "elapsed": 183107,
     "status": "ok",
     "timestamp": 1601459745165,
     "user": {
      "displayName": "Арсен Ринатович Кужамуратов",
      "photoUrl": "",
      "userId": "11945829493768460540"
     },
     "user_tz": -180
    },
    "id": "P6us4E3DGGpl",
    "outputId": "6e7291c0-a83f-4035-ae78-301e1f4cb4ea"
   },
   "outputs": [],
   "source": [
    "# creating tokenizer and downloading embeddings FastText(ru)\n",
    "tokenizer = nltk.tokenize.WordPunctTokenizer()\n",
    "ru_emb = KeyedVectors.load_word2vec_format(\"cc.ru.300.vec\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "5t2S7BunGGpo"
   },
   "outputs": [],
   "source": [
    "# dropping nan values & preparing text and target labels\n",
    "data.dropna(inplace=True)\n",
    "data['class'] = data['class'].apply(lambda x: 1 if x=='true' else 0)\n",
    "data['text'] = data['text'].apply(lambda l: ' '.join(tokenizer.tokenize(str(l))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 282
    },
    "executionInfo": {
     "elapsed": 185295,
     "status": "ok",
     "timestamp": 1601459747976,
     "user": {
      "displayName": "Арсен Ринатович Кужамуратов",
      "photoUrl": "",
      "userId": "11945829493768460540"
     },
     "user_tz": -180
    },
    "id": "DoZHT83FGGps",
    "outputId": "48ff660c-0075-487e-89f9-e9de068cca7e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "### checking text length statistic\n",
    "b = []\n",
    "for text in data['text']:\n",
    "    if isinstance(text, str): \n",
    "        b.append(len(text.split()))\n",
    "plt.hist(b, bins=50)\n",
    "print('Done')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "btV4dbmGGGpx"
   },
   "outputs": [],
   "source": [
    "# creating dict from tokens to their identifiers\n",
    "token_to_id = {word:i for i, word in enumerate(ru_emb.index2word)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "WbZz0lvAGGp1"
   },
   "outputs": [],
   "source": [
    "# UNK for unseen words \n",
    "# PAD for empty words\n",
    "UNK, PAD = 'unknown', 'pad'\n",
    "UNK_IX, PAD_IX = map(token_to_id.get, [UNK, PAD])\n",
    "\n",
    "def as_matrix(sequences, max_len=None):\n",
    "    \"\"\" Convert a list of tokens into a matrix with padding \"\"\"\n",
    "    if isinstance(sequences[0], str):\n",
    "        sequences = list(map(str.split, sequences))   \n",
    "    max_len = min(max(map(len, sequences)), max_len or float('inf'))\n",
    "    matrix = np.full((len(sequences), max_len), np.int32(PAD_IX))\n",
    "    for i,seq in enumerate(sequences):\n",
    "        row_ix = [token_to_id.get(word, UNK_IX) for word in seq[:max_len]]\n",
    "        matrix[i, :len(row_ix)] = row_ix \n",
    "    return matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CbzJvMpSGGp3"
   },
   "source": [
    "# ML part"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 51
    },
    "executionInfo": {
     "elapsed": 184318,
     "status": "ok",
     "timestamp": 1601459748313,
     "user": {
      "displayName": "Арсен Ринатович Кужамуратов",
      "photoUrl": "",
      "userId": "11945829493768460540"
     },
     "user_tz": -180
    },
    "id": "wDDh1HbJGGp4",
    "outputId": "587dbd57-e448-4b55-819b-1314aec8f3b4"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train size =  22684\n",
      "Validation size =  1194\n"
     ]
    }
   ],
   "source": [
    "# spliting data into train and validation sets\n",
    "# train data\n",
    "data_train, data_val = train_test_split(data, test_size=0.05, random_state=42)\n",
    "print(\"Train size = \", len(data_train))\n",
    "print(\"Validation size = \", len(data_val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "3jlUA3nxGGp6"
   },
   "outputs": [],
   "source": [
    "def generate_batch(data, batch_size=None, replace=True, max_len=None):\n",
    "    \"\"\"\n",
    "    Creates a pytorch-friendly dict from the batch data.\n",
    "    :returns: a dict with {'title' : int64[batch, title_max_len]\n",
    "    \"\"\"\n",
    "    if batch_size is not None:\n",
    "        data = data.sample(batch_size, replace=replace)\n",
    "    batch = {}\n",
    "    batch['text'] = as_matrix(data['text'].values, max_len)\n",
    "    batch['class'] = data['class'].values\n",
    "    return batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 119
    },
    "executionInfo": {
     "elapsed": 183737,
     "status": "ok",
     "timestamp": 1601459748314,
     "user": {
      "displayName": "Арсен Ринатович Кужамуратов",
      "photoUrl": "",
      "userId": "11945829493768460540"
     },
     "user_tz": -180
    },
    "id": "tFS06JEoGGp8",
    "outputId": "4b324921-d305-4b8b-f3c3-b4b1328040a0"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'text': array([[   205,    191,     32,    166,     65,   1314,     69,   4607,\n",
      "          4081,     23],\n",
      "       [ 59339,     35,     14,   4705, 130630,  10521,   9223,   4227,\n",
      "         13930,    313],\n",
      "       [  1386,  25481,  25481,  11808,   3137,   3254,  25481,   6252,\n",
      "            19, 379274]], dtype=int32), 'class': array([1, 1, 1])}\n"
     ]
    }
   ],
   "source": [
    "batch = generate_batch(data_train, 3, max_len=10)\n",
    "print(batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "id": "HK9IITOGGGp_"
   },
   "outputs": [],
   "source": [
    "# Different pooling layers\n",
    "\n",
    "class GlobalMaxPooling(nn.Module):\n",
    "    def __init__(self, dim=-1):\n",
    "        super(self.__class__, self).__init__()\n",
    "        self.dim = dim\n",
    "    def forward(self, x):\n",
    "        return x.max(dim=self.dim)[0]\n",
    "    \n",
    "class GlobalAveragePooling(nn.Module):\n",
    "    def __init__(self, dim=-1):\n",
    "        super(self.__class__, self).__init__()\n",
    "        self.dim = dim\n",
    "        \n",
    "    def forward(self, x):\n",
    "        return x.mean(dim=self.dim)\n",
    "\n",
    "class SoftmaxAveragePooling(nn.Module):\n",
    "    def __init__(self, dim=-1):\n",
    "        super(self.__class__, self).__init__()\n",
    "        self.dim = dim\n",
    "    def forward(self,x):\n",
    "        prob = F.softmax(x,dim=self.dim)\n",
    "        return (x*prob).sum(dim=self.dim)\n",
    "    \n",
    "class AttentivePooling(nn.Module):\n",
    "    \n",
    "    def __init__(self, hid_size):\n",
    "        super(self.__class__, self).__init__()\n",
    "        self.dense = nn.Linear(hid_size, 1)\n",
    "        \n",
    "    def forward(self,x):\n",
    "        weights = self.dense(x)\n",
    "        normalized_weights = F.softmax(weights, dim=1)\n",
    "        return torch.sum(x*normalized_weights, dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "id": "7234lLs5GGqB"
   },
   "outputs": [],
   "source": [
    "# In order to use pretrained embeddings\n",
    "# we create this class\n",
    "class EmbeddingLayer(nn.Module):\n",
    "    def __init__(self, device):\n",
    "        super(self.__class__, self).__init__()\n",
    "        self.emb = nn.Embedding(600_000, 300, padding_idx=PAD_IX)\n",
    "        self.emb.weight.data = torch.tensor(ru_emb.vectors).to(device)\n",
    "    def forward(self, x):\n",
    "        return self.emb(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hekixROaZp_q"
   },
   "source": [
    "## Network architecture\n",
    "texts -> Embedder -> bLSTM -> [AttentivePooling]*3 -> dense_layers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"network.png\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Y2_rz5CUGGqE"
   },
   "outputs": [],
   "source": [
    "class Network(nn.Module):\n",
    "    def __init__(self):\n",
    "        \"\"\" \n",
    "        A simple sequential encoder using bidirectional lstm\n",
    "        Pooling on top of them (AttentivePooling: 3)\n",
    "        Everywhere dropout for regularization\n",
    "        \"\"\"\n",
    "        super(self.__class__, self).__init__()\n",
    "        self.hid = 64\n",
    "        self.dr1 = nn.Dropout(0.5)\n",
    "        self.rnn = nn.LSTM(300, self.hid, batch_first=True,bidirectional=True)\n",
    "        self.pool1 = AttentivePooling(2*self.hid)\n",
    "        self.pool2 = AttentivePooling(2*self.hid)\n",
    "        self.pool3 = AttentivePooling(2*self.hid)\n",
    "        self.dr2 = nn.Dropout(0.5)\n",
    "        self.dense1 = nn.Linear(6*self.hid, self.hid)\n",
    "        self.dr3 = nn.Dropout(0.5)\n",
    "        self.dense2 = nn.Linear(self.hid, 2)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.dr1(x)\n",
    "        x, prev = self.rnn(x)\n",
    "        x = torch.cat([self.pool1(x), self.pool2(x), self.pool3(x)], 1)\n",
    "        x = self.dr2(x)\n",
    "        x = F.relu(self.dense1(x))\n",
    "        x = self.dr3(x)\n",
    "        return self.dense2(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 51
    },
    "executionInfo": {
     "elapsed": 184956,
     "status": "ok",
     "timestamp": 1601459751031,
     "user": {
      "displayName": "Арсен Ринатович Кужамуратов",
      "photoUrl": "",
      "userId": "11945829493768460540"
     },
     "user_tz": -180
    },
    "id": "j8C92myTGGqG",
    "outputId": "a6b39b6f-5b92-47d4-8840-0a20d95f0f31"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([3, 195])\n",
      "Seems fine\n"
     ]
    }
   ],
   "source": [
    "# Sanity check\n",
    "Embedder = EmbeddingLayer('cpu')\n",
    "title_encoder = Network()\n",
    "\n",
    "dummy_x = torch.LongTensor(generate_batch(data_train, 3)['text'])\n",
    "print(dummy_x.shape)\n",
    "dummy_v = title_encoder(Embedder(dummy_x))\n",
    "\n",
    "assert tuple(dummy_v.shape) == (dummy_x.shape[0], 2)\n",
    "\n",
    "del title_encoder\n",
    "print(\"Seems fine\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "365-DVVRGGqI"
   },
   "outputs": [],
   "source": [
    "# if cuda available use \"cuda\" else \"cpu\"\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "Embedder = EmbeddingLayer(device)\n",
    "model = Network().to(device)\n",
    "opt = torch.optim.Adam(model.parameters(), lr=1e-3)\n",
    "compute_loss = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "IBg2Q9WmGGqK"
   },
   "outputs": [],
   "source": [
    "def iterate_minibatches(data, batch_size=32, max_len=None,\n",
    "                        max_batches=None, shuffle=True, verbose=True):\n",
    "    \"\"\"\n",
    "    Iterator over minibatches\n",
    "    returns: batch of data\n",
    "    \"\"\"\n",
    "    indices = np.arange(len(data))\n",
    "    if shuffle:\n",
    "        indices = np.random.permutation(indices)\n",
    "    if max_batches is not None:\n",
    "        indices = indices[: batch_size * max_batches]\n",
    "        \n",
    "    irange = tnrange if verbose else range\n",
    "    for start in irange(0, len(indices), batch_size):\n",
    "        yield generate_batch(data.iloc[indices[start : start + batch_size]], max_len=max_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "-dGGMIo0GGqM"
   },
   "outputs": [],
   "source": [
    "# training parameters\n",
    "num_epochs = 100\n",
    "max_len = 450\n",
    "batch_size = 64\n",
    "batches_per_epoch = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ucZeqWogGGqO"
   },
   "outputs": [],
   "source": [
    "def compute_score(prediction, reference):\n",
    "    \n",
    "  \"\"\"\n",
    "     Computing f1 score on minibatch \n",
    "     returns: f1_score numpy array\n",
    "  \"\"\"\n",
    "    res_pred = F.softmax(prediction, dim=-1).argmax(1).cpu().numpy()\n",
    "    res_true = reference.cpu().numpy()\n",
    "    return f1_score(res_true, res_pred, average='macro')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NzMacm-tYwHv"
   },
   "source": [
    "## Training loop\n",
    "__1 step__ - train only encoding network with __fixed__ input embeddings lr = 1e-3 Adam\n",
    "\n",
    "__2 step__ - __finetune__ end-to-end encoding network + pretrained embeddings lr = 1e-4 Adam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "executionInfo": {
     "elapsed": 2165998,
     "status": "ok",
     "timestamp": 1601461734047,
     "user": {
      "displayName": "Арсен Ринатович Кужамуратов",
      "photoUrl": "",
      "userId": "11945829493768460540"
     },
     "user_tz": -180
    },
    "id": "CKoTLFEFGGqT",
    "outputId": "15c97a74-0f60-4583-b702-a76354fe808a",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iter 0 train_loss = 0.47846045315265656 train_f1 = 0.45550703126900527\n",
      "iter 0 val_loss = 0.40979525760600444 val_f1 = 0.4556134073634812\n",
      "iter 1 train_loss = 0.3586019589006901 train_f1 = 0.5271190908181428\n",
      "iter 1 val_loss = 0.28205705708579015 val_f1 = 0.8105117701444022\n",
      "iter 2 train_loss = 0.226918307505548 train_f1 = 0.8058351621950401\n",
      "iter 2 val_loss = 0.14674165207696588 val_f1 = 0.8607229866087271\n",
      "iter 3 train_loss = 0.1510003624856472 train_f1 = 0.8767898795911566\n",
      "iter 3 val_loss = 0.16747220963435738 val_f1 = 0.8890300945175853\n",
      "iter 4 train_loss = 0.12212865186855197 train_f1 = 0.9188872124705573\n",
      "iter 4 val_loss = 0.08477752851812463 val_f1 = 0.9269974570782552\n",
      "iter 5 train_loss = 0.1003018338046968 train_f1 = 0.9226844240731518\n",
      "iter 5 val_loss = 0.07676847566433839 val_f1 = 0.9436159454326625\n",
      "iter 6 train_loss = 0.09226979120634496 train_f1 = 0.9308366587542565\n",
      "iter 6 val_loss = 0.06364791551114697 val_f1 = 0.9575109493283815\n",
      "iter 7 train_loss = 0.09539536006283016 train_f1 = 0.935752520054647\n",
      "iter 7 val_loss = 0.05213952388026213 val_f1 = 0.9590050518292559\n",
      "iter 8 train_loss = 0.07665294008329511 train_f1 = 0.9436626569718222\n",
      "iter 8 val_loss = 0.05096086361568029 val_f1 = 0.9501897979175898\n",
      "iter 9 train_loss = 0.08075152166886256 train_f1 = 0.9303215113672009\n",
      "iter 9 val_loss = 0.06578052420120098 val_f1 = 0.9294766990677296\n",
      "iter 10 train_loss = 0.07103894154541195 train_f1 = 0.9567437490204059\n",
      "iter 10 val_loss = 0.04836102746623127 val_f1 = 0.9638482505788213\n",
      "iter 11 train_loss = 0.06212170027196407 train_f1 = 0.952287111415336\n",
      "iter 11 val_loss = 0.0663250587807086 val_f1 = 0.9570585730174032\n",
      "iter 12 train_loss = 0.06340368274832145 train_f1 = 0.9647696037911347\n",
      "iter 12 val_loss = 0.03530676658939276 val_f1 = 0.9680841083468207\n",
      "iter 13 train_loss = 0.06779357303865254 train_f1 = 0.9565588149314113\n",
      "iter 13 val_loss = 0.039988722076247395 val_f1 = 0.9754287057826414\n",
      "iter 14 train_loss = 0.06380059910472483 train_f1 = 0.9500399705366747\n",
      "iter 14 val_loss = 0.03945746895930681 val_f1 = 0.9747065636586868\n",
      "iter 15 train_loss = 0.05236107605160214 train_f1 = 0.9647983367067969\n",
      "iter 15 val_loss = 0.03687070524105557 val_f1 = 0.9748907044812158\n",
      "iter 16 train_loss = 0.05967150401440449 train_f1 = 0.9644772004153299\n",
      "iter 16 val_loss = 0.04220526209897607 val_f1 = 0.9723114855234949\n",
      "iter 17 train_loss = 0.05560035113012418 train_f1 = 0.963169291836769\n",
      "iter 17 val_loss = 0.0649847756139934 val_f1 = 0.958803133596328\n",
      "iter 18 train_loss = 0.052807458702009175 train_f1 = 0.9671694434896875\n",
      "iter 18 val_loss = 0.028926579087151606 val_f1 = 0.9830321634964044\n",
      "iter 19 train_loss = 0.0509666976169683 train_f1 = 0.963247986786082\n",
      "iter 19 val_loss = 0.047431585464724584 val_f1 = 0.9718324845822897\n",
      "iter 20 train_loss = 0.04335758920526132 train_f1 = 0.9712109304186446\n",
      "iter 20 val_loss = 0.038768036781173 val_f1 = 0.9760525376355301\n",
      "iter 21 train_loss = 0.038940505224163646 train_f1 = 0.9720993118869063\n",
      "iter 21 val_loss = 0.02842560097403628 val_f1 = 0.9853156625262086\n",
      "iter 22 train_loss = 0.04356912835384719 train_f1 = 0.9684030812228177\n",
      "iter 22 val_loss = 0.04637640379032267 val_f1 = 0.9708993955287837\n",
      "iter 23 train_loss = 0.049064090189058336 train_f1 = 0.964733260653948\n",
      "iter 23 val_loss = 0.030291843007465725 val_f1 = 0.9800452032823312\n",
      "iter 24 train_loss = 0.05081831706454978 train_f1 = 0.9636398095185423\n",
      "iter 24 val_loss = 0.02697835494166755 val_f1 = 0.9786114741259113\n",
      "iter 25 train_loss = 0.043154347017698456 train_f1 = 0.9688872846810301\n",
      "iter 25 val_loss = 0.08545893376791164 val_f1 = 0.9504614016407743\n",
      "iter 26 train_loss = 0.037211766063701364 train_f1 = 0.9816878274120288\n",
      "iter 26 val_loss = 0.027621879502763284 val_f1 = 0.9827753751562\n",
      "iter 27 train_loss = 0.04101107039954513 train_f1 = 0.9673393548593603\n",
      "iter 27 val_loss = 0.037096971713349615 val_f1 = 0.9727160339147284\n",
      "iter 28 train_loss = 0.03848814229248092 train_f1 = 0.9663915864598934\n",
      "iter 28 val_loss = 0.03514598426715422 val_f1 = 0.9806083545017842\n",
      "iter 29 train_loss = 0.042077538808807734 train_f1 = 0.9692749517191358\n",
      "iter 29 val_loss = 0.026729353976215384 val_f1 = 0.9840455188412043\n",
      "iter 30 train_loss = 0.03869334000890376 train_f1 = 0.9759483571090678\n",
      "iter 30 val_loss = 0.02137090883824337 val_f1 = 0.9858867576088038\n",
      "iter 31 train_loss = 0.03158748153888155 train_f1 = 0.9803771894129711\n",
      "iter 31 val_loss = 0.01963437479035316 val_f1 = 0.9873311920224538\n",
      "iter 32 train_loss = 0.036325434227474036 train_f1 = 0.9767620553517365\n",
      "iter 32 val_loss = 0.017618923242767585 val_f1 = 0.9839996676027702\n",
      "iter 33 train_loss = 0.03479046930500772 train_f1 = 0.9760567535810529\n",
      "iter 33 val_loss = 0.03522411763429103 val_f1 = 0.9772967195188097\n",
      "iter 34 train_loss = 0.02037678692904592 train_f1 = 0.97676533606909\n",
      "iter 34 val_loss = 0.013230491997857922 val_f1 = 0.9919467276156212\n",
      "iter 35 train_loss = 0.029269054985197728 train_f1 = 0.9784848712720349\n",
      "iter 35 val_loss = 0.014521003034659722 val_f1 = 0.9870324790552772\n",
      "iter 36 train_loss = 0.04199781071569305 train_f1 = 0.9658467543750665\n",
      "iter 36 val_loss = 0.036990810173416606 val_f1 = 0.9745874980772872\n",
      "iter 37 train_loss = 0.03149850501387846 train_f1 = 0.978028490973159\n",
      "iter 37 val_loss = 0.015944103544921075 val_f1 = 0.9866861184656276\n",
      "iter 38 train_loss = 0.03793454964106786 train_f1 = 0.9697858023824444\n",
      "iter 38 val_loss = 0.036807885240322274 val_f1 = 0.9729953977378198\n",
      "iter 39 train_loss = 0.0379600170039339 train_f1 = 0.972401567519008\n",
      "iter 39 val_loss = 0.023335974491170707 val_f1 = 0.9806648356495385\n",
      "iter 40 train_loss = 0.02984659676465526 train_f1 = 0.974221489635455\n",
      "iter 40 val_loss = 0.042798557859465576 val_f1 = 0.972272387784941\n",
      "iter 41 train_loss = 0.026230002295051236 train_f1 = 0.9812239918109792\n",
      "iter 41 val_loss = 0.022411971104845992 val_f1 = 0.9840455188412043\n",
      "iter 42 train_loss = 0.03916774880257435 train_f1 = 0.9680956241347995\n",
      "iter 42 val_loss = 0.01869626993031584 val_f1 = 0.9833447439520605\n",
      "iter 43 train_loss = 0.03383902326371754 train_f1 = 0.9772242597969413\n",
      "iter 43 val_loss = 0.017973382487023053 val_f1 = 0.9833523938641323\n",
      "iter 44 train_loss = 0.02535343873387319 train_f1 = 0.9803843952024404\n",
      "iter 44 val_loss = 0.03983251088663421 val_f1 = 0.9752091046224988\n",
      "iter 45 train_loss = 0.018438332347950562 train_f1 = 0.9887521386769673\n",
      "iter 45 val_loss = 0.0171224508979465 val_f1 = 0.9833603377272747\n",
      "iter 46 train_loss = 0.02664794269570848 train_f1 = 0.9782035558718402\n",
      "iter 46 val_loss = 0.01997355848135684 val_f1 = 0.9870168852800629\n",
      "iter 47 train_loss = 0.023166355999346706 train_f1 = 0.9853551607510498\n",
      "iter 47 val_loss = 0.03256968250283726 val_f1 = 0.9771019819196446\n",
      "iter 48 train_loss = 0.02728068684489699 train_f1 = 0.9824827229746553\n",
      "iter 48 val_loss = 0.014428461309235948 val_f1 = 0.9874694817494643\n",
      "iter 49 train_loss = 0.023631169467116707 train_f1 = 0.98224125200834\n",
      "iter 49 val_loss = 0.015989761194903526 val_f1 = 0.9908108562630316\n",
      "iter 50 train_loss = 0.027618102680899027 train_f1 = 0.9810491827043955\n",
      "iter 50 val_loss = 0.01121502404911505 val_f1 = 0.9905022932019713\n",
      "iter 51 train_loss = 0.017965267101317295 train_f1 = 0.9855353976267918\n",
      "iter 51 val_loss = 0.011629110529994298 val_f1 = 0.9919467276156212\n",
      "iter 52 train_loss = 0.011798193665526924 train_f1 = 0.986715336024493\n",
      "iter 52 val_loss = 0.013271818760921107 val_f1 = 0.9892165557417526\n",
      "iter 53 train_loss = 0.008275098199665081 train_f1 = 0.9947969159256854\n",
      "iter 53 val_loss = 0.01189476994946553 val_f1 = 0.9932168713006256\n",
      "iter 54 train_loss = 0.011424800374716142 train_f1 = 0.9937704438473736\n",
      "iter 54 val_loss = 0.011343071104288595 val_f1 = 0.9920711498541523\n",
      "iter 55 train_loss = 0.00926961081450827 train_f1 = 0.993040113780945\n",
      "iter 55 val_loss = 0.01408402346777094 val_f1 = 0.9892165557417526\n",
      "iter 56 train_loss = 0.00486161764575968 train_f1 = 0.9961220562883562\n",
      "iter 56 val_loss = 0.014524780648367883 val_f1 = 0.9920711498541523\n",
      "iter 57 train_loss = 0.006213659876830206 train_f1 = 0.9962353310506823\n",
      "iter 57 val_loss = 0.009005462531427767 val_f1 = 0.9919467276156212\n",
      "iter 58 train_loss = 0.004237265815269211 train_f1 = 0.9975672711141573\n",
      "iter 58 val_loss = 0.01050411093405719 val_f1 = 0.9932168713006256\n",
      "iter 59 train_loss = 0.006202350365294933 train_f1 = 0.9949259310136078\n",
      "iter 59 val_loss = 0.011758913247138923 val_f1 = 0.9920711498541523\n",
      "iter 60 train_loss = 0.0046055436192921914 train_f1 = 0.9976194650911633\n",
      "iter 60 val_loss = 0.02341517473888681 val_f1 = 0.9882593875936365\n",
      "iter 61 train_loss = 0.004413725262932067 train_f1 = 0.998332801701223\n",
      "iter 61 val_loss = 0.014405057434200905 val_f1 = 0.9932168713006256\n",
      "iter 62 train_loss = 0.009024687185284454 train_f1 = 0.9948680173302107\n",
      "iter 62 val_loss = 0.013992929984370102 val_f1 = 0.99491809405448\n",
      "iter 63 train_loss = 0.008905149124825477 train_f1 = 0.9957585019532457\n",
      "iter 63 val_loss = 0.01454977532325982 val_f1 = 0.99491809405448\n",
      "iter 64 train_loss = 0.002058249144359858 train_f1 = 0.9980688757407025\n",
      "iter 64 val_loss = 0.011535989777837319 val_f1 = 0.9936479503694756\n",
      "iter 65 train_loss = 0.003413000755419107 train_f1 = 0.9979596459171869\n",
      "iter 65 val_loss = 0.02869788025803143 val_f1 = 0.9869892439086322\n",
      "iter 66 train_loss = 0.004928464484054557 train_f1 = 0.9975159220495192\n",
      "iter 66 val_loss = 0.02739986639988214 val_f1 = 0.9882593875936365\n",
      "iter 67 train_loss = 0.00342467926643234 train_f1 = 0.9980026452638171\n",
      "iter 67 val_loss = 0.014730084889267817 val_f1 = 0.9936479503694756\n",
      "iter 68 train_loss = 0.002987412973735628 train_f1 = 0.9974520834458156\n",
      "iter 68 val_loss = 0.017536562739213474 val_f1 = 0.9936479503694756\n",
      "iter 69 train_loss = 0.0016068214551339 train_f1 = 0.9972198264427299\n",
      "iter 69 val_loss = 0.03306785051311247 val_f1 = 0.9872121265839695\n",
      "iter 70 train_loss = 0.004174353075497545 train_f1 = 0.9983959899749374\n",
      "iter 70 val_loss = 0.01618238551067823 val_f1 = 0.9936479503694756\n",
      "iter 71 train_loss = 0.0008614400288880075 train_f1 = 1.0\n",
      "iter 71 val_loss = 0.016317578250507905 val_f1 = 0.9936479503694756\n",
      "iter 72 train_loss = 0.002489905568128925 train_f1 = 0.9983065700812886\n",
      "iter 72 val_loss = 0.024927284614932533 val_f1 = 0.9916618331013453\n",
      "iter 73 train_loss = 0.011068569785851173 train_f1 = 0.9963489102106446\n",
      "iter 73 val_loss = 0.02209301967121795 val_f1 = 0.9937723726080065\n",
      "iter 74 train_loss = 3.541931523705077e-05 train_f1 = 1.0\n",
      "iter 74 val_loss = 0.022117701212392843 val_f1 = 0.99491809405448\n",
      "iter 75 train_loss = 0.00328663808247861 train_f1 = 0.9985608799294253\n",
      "iter 75 val_loss = 0.02547370469396422 val_f1 = 0.9937723726080065\n",
      "iter 76 train_loss = 0.00019132042365471057 train_f1 = 1.0\n",
      "iter 76 val_loss = 0.020443707024045896 val_f1 = 0.9937723726080065\n",
      "iter 77 train_loss = 0.006455939715796608 train_f1 = 0.9990346907993967\n",
      "iter 77 val_loss = 0.021954739722519805 val_f1 = 0.9937723726080065\n",
      "iter 78 train_loss = 1.721226581407498e-05 train_f1 = 1.0\n",
      "iter 78 val_loss = 0.018025848469048842 val_f1 = 0.9961882377394843\n",
      "iter 79 train_loss = 0.0025970490854355433 train_f1 = 0.9985637868960129\n",
      "iter 79 val_loss = 0.036666704819555786 val_f1 = 0.9903699271002978\n",
      "iter 80 train_loss = 1.2766535279236101e-05 train_f1 = 1.0\n",
      "iter 80 val_loss = 0.016966130716769164 val_f1 = 0.9936479503694756\n",
      "iter 81 train_loss = 0.0005013025695310524 train_f1 = 0.9991979949874686\n",
      "iter 81 val_loss = 0.019188337193133533 val_f1 = 0.9936479503694756\n",
      "iter 82 train_loss = 6.01253577293015e-06 train_f1 = 1.0\n",
      "iter 82 val_loss = 0.057583075972861074 val_f1 = 0.9869892439086322\n",
      "iter 83 train_loss = 0.00048030534723031204 train_f1 = 0.9994511149228131\n",
      "iter 83 val_loss = 0.026654648329973243 val_f1 = 0.9932168713006256\n",
      "iter 84 train_loss = 0.0015649008484976345 train_f1 = 0.9994511149228131\n",
      "iter 84 val_loss = 0.04161835839585676 val_f1 = 0.989960610347491\n",
      "iter 85 train_loss = 0.0035391573436048063 train_f1 = 0.9983621202120267\n",
      "iter 85 val_loss = 0.02559236346889669 val_f1 = 0.99491809405448\n",
      "iter 86 train_loss = 0.003558778959441313 train_f1 = 0.999050614297031\n",
      "iter 86 val_loss = 0.02844335254085986 val_f1 = 0.99448701498563\n",
      "iter 87 train_loss = 0.006224851906158275 train_f1 = 0.9985515303410041\n",
      "iter 87 val_loss = 0.028441182993041565 val_f1 = 0.9932168713006256\n",
      "iter 88 train_loss = 8.570920758782074e-05 train_f1 = 1.0\n",
      "iter 88 val_loss = 0.027753396211383397 val_f1 = 0.99448701498563\n",
      "iter 89 train_loss = 0.0009846118247106284 train_f1 = 0.9993535353535353\n",
      "iter 89 val_loss = 0.032491907646235385 val_f1 = 0.99448701498563\n",
      "iter 90 train_loss = 1.5224458862805878e-05 train_f1 = 1.0\n",
      "iter 90 val_loss = 0.02624555959203084 val_f1 = 0.99491809405448\n",
      "iter 91 train_loss = 2.7895198034233458e-05 train_f1 = 1.0\n",
      "iter 91 val_loss = 0.027408301137311443 val_f1 = 0.99448701498563\n",
      "iter 92 train_loss = 0.0003916867392787693 train_f1 = 1.0\n",
      "iter 92 val_loss = 0.03716365641787723 val_f1 = 0.9932168713006256\n",
      "iter 93 train_loss = 0.006709253825423811 train_f1 = 0.9979477266701987\n",
      "iter 93 val_loss = 0.03314381856437961 val_f1 = 0.9932168713006256\n",
      "iter 94 train_loss = 0.0026920665835808454 train_f1 = 0.9985409852314402\n",
      "iter 94 val_loss = 0.05168451606320632 val_f1 = 0.9898361881089599\n",
      "iter 95 train_loss = 3.2820410646783316e-05 train_f1 = 1.0\n",
      "iter 95 val_loss = 0.034329873440899904 val_f1 = 0.9932168713006256\n",
      "iter 96 train_loss = 0.006853782384960658 train_f1 = 0.9984355066035739\n",
      "iter 96 val_loss = 0.024773003635145615 val_f1 = 0.9961882377394843\n",
      "iter 97 train_loss = 0.003403059703061708 train_f1 = 0.9994511149228131\n",
      "iter 97 val_loss = 0.05355763378474734 val_f1 = 0.989960610347491\n",
      "iter 98 train_loss = 6.550316258580047e-07 train_f1 = 1.0\n",
      "iter 98 val_loss = 0.051494046936028305 val_f1 = 0.989960610347491\n",
      "iter 99 train_loss = 0.00010014334421524662 train_f1 = 1.0\n",
      "iter 99 val_loss = 0.048819001284329755 val_f1 = 0.989960610347491\n"
     ]
    }
   ],
   "source": [
    "for epoch_i in range(num_epochs):\n",
    "    \n",
    "    #print(\"Training:\")\n",
    "    train_loss = train_f1 = train_batches = 0    \n",
    "    model.train(True)\n",
    "    if epoch_i > 50:\n",
    "        opt = torch.optim.Adam(itertools.chain(*[Embedder.parameters(), model.parameters()]), lr=1e-4)\n",
    "    for batch in iterate_minibatches(data_train, max_batches=batches_per_epoch,verbose = False):\n",
    "\n",
    "        title_ix = torch.LongTensor(batch[\"text\"]).to(device)\n",
    "        reference = torch.LongTensor(batch[\"class\"]).to(device)\n",
    "        prediction = model(Embedder(title_ix))\n",
    "        loss = compute_loss(prediction, reference)\n",
    "        loss.backward()\n",
    "        opt.step()\n",
    "        opt.zero_grad()\n",
    "        train_loss += loss.detach().cpu().numpy()\n",
    "        train_f1 += compute_score(prediction, reference)\n",
    "        train_batches += 1\n",
    "    \n",
    "    print('iter {} train_loss = {} train_f1 = {}'.format(epoch_i,train_loss/train_batches, train_f1/train_batches))\n",
    "    \n",
    "    #print(\"Validation:\")\n",
    "    val_loss = val_f1 = val_batches = 0\n",
    "    model.train(False)\n",
    "    \n",
    "    for batch in iterate_minibatches(data_val, shuffle=False,verbose= False):\n",
    "        title_ix = torch.LongTensor(batch[\"text\"]).to(device)\n",
    "        reference = torch.LongTensor(batch[\"class\"]).to(device)\n",
    "\n",
    "        prediction = model(Embedder(title_ix))\n",
    "        loss = compute_loss(prediction, reference)\n",
    "        val_loss += loss.detach().cpu().numpy()\n",
    "        val_f1 += compute_score(prediction, reference)\n",
    "        val_batches += 1\n",
    "    \n",
    "    print('iter {} val_loss = {} val_f1 = {}'.format(epoch_i,val_loss/val_batches, val_f1/val_batches))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "446wET9FGGqV"
   },
   "source": [
    "# Create submission file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "kwbC6YEnGGqV"
   },
   "outputs": [],
   "source": [
    "data_test = pd.read_csv('test.csv', sep='\\t')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "od46oO2wGGqX"
   },
   "outputs": [],
   "source": [
    "data_test['text'] = data_test['text'].apply(lambda l: ' '.join(tokenizer.tokenize(str(l))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "4xVj88U0GGqZ"
   },
   "outputs": [],
   "source": [
    "data_as_matrix = as_matrix(data_test['text'], max_len=max_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "4PFi_DFpGGqe"
   },
   "outputs": [],
   "source": [
    "from tqdm import trange"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "executionInfo": {
     "elapsed": 2576814,
     "status": "ok",
     "timestamp": 1601462150174,
     "user": {
      "displayName": "Арсен Ринатович Кужамуратов",
      "photoUrl": "",
      "userId": "11945829493768460540"
     },
     "user_tz": -180
    },
    "id": "s5cooxvVGGqh",
    "outputId": "a01a73af-38a1-4817-9219-3fdb37aa6b80"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 6024/6024 [06:53<00:00, 14.57it/s]\n"
     ]
    }
   ],
   "source": [
    "# You can change to 'cuda' \n",
    "# I trained on my own gpu with low memory so i use 'cpu' here\n",
    "model.train(False)\n",
    "predictions = torch.zeros(len(data_test),2)\n",
    "test = torch.LongTensor(data_as_matrix)\n",
    "model = model.to('cpu')\n",
    "Embedder = Embedder.to('cpu')\n",
    "for i in trange(len(data_test)):\n",
    "    predictions[i,:] = F.softmax(model(Embedder(test[i].unsqueeze(0)))[0],-1).detach()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "U8EWHzE1GGqj"
   },
   "outputs": [],
   "source": [
    "solutions = predictions.argmax(1).numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "YuSMLJkGGGql"
   },
   "outputs": [],
   "source": [
    "columns = data_test.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 187
    },
    "executionInfo": {
     "elapsed": 2023,
     "status": "ok",
     "timestamp": 1601462152224,
     "user": {
      "displayName": "Арсен Ринатович Кужамуратов",
      "photoUrl": "",
      "userId": "11945829493768460540"
     },
     "user_tz": -180
    },
    "id": "7pkbcn5HGGqp",
    "outputId": "7aff7711-39bd-4a96-d836-bd21048eb046"
   },
   "outputs": [],
   "source": [
    "for i in range(len(data_test)):\n",
    "    data_test[columns[1]][i] = 'true' if solutions[i]==1 else 'fake' "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "XSXz1RLmGGqt"
   },
   "outputs": [],
   "source": [
    "data_test.to_csv('test_solution.csv', sep='\\t')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ohuYVVnFGGq8"
   },
   "outputs": [],
   "source": [
    "# Attention!!!!!\n",
    "# Rename 'test_solution.csv' to 'test.csv' and zip it to 'test.zip'.\n",
    "# Then send it to checking system"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Final Score = 0.98951"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Remarks on chosen methods"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Since size of dataset is small enough I tried to use pretrained embeddings/Tfidf feature extractor\n",
    "and combine it with classical methods such as RandomForest or GradientBoosting.\n",
    "Results wasn't promising, so I switch to neural networks and main efforts \n",
    "was at fighting overfitting and unbalance in targets.__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__tokenizer__ (small effect)\n",
    "\n",
    "--I tried several tokenizers: with lemmatization, deleting stopwords and punctuation   \n",
    "\n",
    "__other embeddings__ (small effect)\n",
    "\n",
    "-- I tried other russian embeddings from github \n",
    "\n",
    "-- Here I finetune it, so it effected on quality rather small\n",
    "\n",
    "__Max_len__ (small effect)\n",
    "\n",
    "-- Changing max_len also give small effect\n",
    "\n",
    "__Unbalanced targets__ (small effect)\n",
    "\n",
    "-- Tried reweight classes in losses\n",
    "\n",
    "-- Tried to sample balanced (sample fakes more often)\n",
    "\n",
    "__Network architecture__\n",
    "\n",
    "-- First attempt based on Conv1d network with AveragePooling instead LSTM with AttentionPooling\n",
    "\n",
    "-- Main recipe: use bLSTM(or GRU) with several attention pooling\n",
    "\n",
    "-- To avoid overfitting I used dropout. I tried weight_decay but dropout gain better results\n",
    "\n",
    "Dataset was small enough to train embeddings from scratch\n",
    "\n",
    "__Training parameters__\n",
    "\n",
    "-- To accelerate training I used minibatch and Adam optimizer.Training was on GPU 1050Ti (4Gb Ram)\n",
    "\n",
    "-- I tried to change batch_size and epoch from which finetune was started (small effect)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TO DO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__1) Use transformer pretrained models (BERT from deeppavlov for rus)__\n",
    "\n",
    "__2) Try to Ensembling/Stacking models__\n",
    "\n",
    "__3) Augment somehow data (reordering??)__\n",
    "\n",
    "__4) Use label smoothing__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "task2.ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
